{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-03-12T08:02:47.226655Z","iopub.execute_input":"2023-03-12T08:02:47.227497Z","iopub.status.idle":"2023-03-12T08:02:47.292286Z","shell.execute_reply.started":"2023-03-12T08:02:47.227453Z","shell.execute_reply":"2023-03-12T08:02:47.291356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n!pip install jiwer\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport matplotlib.pyplot as plt\nfrom IPython import display\nfrom jiwer import wer","metadata":{"execution":{"iopub.status.busy":"2023-03-12T08:02:57.48664Z","iopub.execute_input":"2023-03-12T08:02:57.48733Z","iopub.status.idle":"2023-03-12T08:03:24.681718Z","shell.execute_reply.started":"2023-03-12T08:02:57.487293Z","shell.execute_reply":"2023-03-12T08:03:24.680445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_url = \"https://data.keithito.com/data/speech/LJSpeech-1.1.tar.bz2\"\ndata_path = keras.utils.get_file(\"LJSpeech-1.1\", data_url, untar=True)\nwavs_path = data_path + \"/wavs/\"\nmetadata_path = data_path + \"/metadata.csv\"\n\n\n# Read metadata file and parse it\nmetadata_df = pd.read_csv(metadata_path, sep=\"|\", header=None, quoting=3)\nmetadata_df.columns = [\"file_name\", \"transcription\", \"normalized_transcription\"]\nmetadata_df = metadata_df[[\"file_name\", \"normalized_transcription\"]]\nmetadata_df = metadata_df.sample(frac=1).reset_index(drop=True)\nmetadata_df.head(3)","metadata":{"execution":{"iopub.status.busy":"2023-03-12T08:03:29.335253Z","iopub.execute_input":"2023-03-12T08:03:29.336184Z","iopub.status.idle":"2023-03-12T08:09:21.566271Z","shell.execute_reply.started":"2023-03-12T08:03:29.336131Z","shell.execute_reply":"2023-03-12T08:09:21.564213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"split = int(len(metadata_df) * 0.90)\ndf_train = metadata_df[:split]\ndf_val = metadata_df[split:]\n\nprint(f\"Size of the training set: {len(df_train)}\")\nprint(f\"Size of the training set: {len(df_val)}\")","metadata":{"execution":{"iopub.status.busy":"2023-03-12T08:09:26.833352Z","iopub.execute_input":"2023-03-12T08:09:26.833731Z","iopub.status.idle":"2023-03-12T08:09:26.841072Z","shell.execute_reply.started":"2023-03-12T08:09:26.833696Z","shell.execute_reply":"2023-03-12T08:09:26.839838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The set of characters accepted in the transcription.\ncharacters = [x for x in \"abcdefghijklmnopqrstuvwxyz'?! \"]\n# Mapping characters to integers\nchar_to_num = keras.layers.StringLookup(vocabulary=characters, oov_token=\"\")\n# Mapping integers back to original characters\nnum_to_char = keras.layers.StringLookup(\n    vocabulary=char_to_num.get_vocabulary(), oov_token=\"\", invert=True\n)\n\nprint(\n    f\"The vocabulary is: {char_to_num.get_vocabulary()} \"\n    f\"(size ={char_to_num.vocabulary_size()})\"\n)","metadata":{"execution":{"iopub.status.busy":"2023-03-12T08:09:44.241068Z","iopub.execute_input":"2023-03-12T08:09:44.242304Z","iopub.status.idle":"2023-03-12T08:09:49.779116Z","shell.execute_reply.started":"2023-03-12T08:09:44.242265Z","shell.execute_reply":"2023-03-12T08:09:49.778043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# An integer scalar Tensor. The window length in samples.\nframe_length = 256\n# An integer scalar Tensor. The number of samples to step.\nframe_step = 160\n# An integer scalar Tensor. The size of the FFT to apply.\n# If not provided, uses the smallest power of 2 enclosing frame_length.\nfft_length = 384\n\n\ndef encode_single_sample(wav_file, label):\n    ###########################################\n    ##  Process the Audio\n    ##########################################\n    # 1. Read wav file\n    file = tf.io.read_file(wavs_path + wav_file + \".wav\")\n    # 2. Decode the wav file\n    audio, _ = tf.audio.decode_wav(file)\n    audio = tf.squeeze(audio, axis=-1)\n    # 3. Change type to float\n    audio = tf.cast(audio, tf.float32)\n    # 4. Get the spectrogram\n    spectrogram = tf.signal.stft(\n        audio, frame_length=frame_length, frame_step=frame_step, fft_length=fft_length\n    )\n    # 5. We only need the magnitude, which can be derived by applying tf.abs\n    spectrogram = tf.abs(spectrogram)\n    spectrogram = tf.math.pow(spectrogram, 0.5)\n    # 6. normalisation\n    means = tf.math.reduce_mean(spectrogram, 1, keepdims=True)\n    stddevs = tf.math.reduce_std(spectrogram, 1, keepdims=True)\n    spectrogram = (spectrogram - means) / (stddevs + 1e-10)\n    ###########################################\n    ##  Process the label\n    ##########################################\n    # 7. Convert label to Lower case\n    label = tf.strings.lower(label)\n    # 8. Split the label\n    label = tf.strings.unicode_split(label, input_encoding=\"UTF-8\")\n    # 9. Map the characters in label to numbers\n    label = char_to_num(label)\n    # 10. Return a dict as our model is expecting two inputs\n    return spectrogram, label","metadata":{"execution":{"iopub.status.busy":"2023-03-12T08:09:53.849463Z","iopub.execute_input":"2023-03-12T08:09:53.849998Z","iopub.status.idle":"2023-03-12T08:09:53.867355Z","shell.execute_reply.started":"2023-03-12T08:09:53.849948Z","shell.execute_reply":"2023-03-12T08:09:53.865424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 32\n# Define the trainig dataset\ntrain_dataset = tf.data.Dataset.from_tensor_slices(\n    (list(df_train[\"file_name\"]), list(df_train[\"normalized_transcription\"]))\n)\ntrain_dataset = (\n    train_dataset.map(encode_single_sample, num_parallel_calls=tf.data.AUTOTUNE)\n    .padded_batch(batch_size)\n    .prefetch(buffer_size=tf.data.AUTOTUNE)\n)\n\n# Define the validation dataset\nvalidation_dataset = tf.data.Dataset.from_tensor_slices(\n    (list(df_val[\"file_name\"]), list(df_val[\"normalized_transcription\"]))\n)\nvalidation_dataset = (\n    validation_dataset.map(encode_single_sample, num_parallel_calls=tf.data.AUTOTUNE)\n    .padded_batch(batch_size)\n    .prefetch(buffer_size=tf.data.AUTOTUNE)\n)","metadata":{"execution":{"iopub.status.busy":"2023-03-12T08:09:59.888986Z","iopub.execute_input":"2023-03-12T08:09:59.889642Z","iopub.status.idle":"2023-03-12T08:10:00.421806Z","shell.execute_reply.started":"2023-03-12T08:09:59.889605Z","shell.execute_reply":"2023-03-12T08:10:00.420801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(8, 5))\nfor batch in train_dataset.take(1):\n    spectrogram = batch[0][0].numpy()\n    spectrogram = np.array([np.trim_zeros(x) for x in np.transpose(spectrogram)])\n    label = batch[1][0]\n    # Spectrogram\n    label = tf.strings.reduce_join(num_to_char(label)).numpy().decode(\"utf-8\")\n    ax = plt.subplot(2, 1, 1)\n    ax.imshow(spectrogram, vmax=1)\n    ax.set_title(label)\n    ax.axis(\"off\")\n    # Wav\n    file = tf.io.read_file(wavs_path + list(df_train[\"file_name\"])[0] + \".wav\")\n    audio, _ = tf.audio.decode_wav(file)\n    audio = audio.numpy()\n    ax = plt.subplot(2, 1, 2)\n    plt.plot(audio)\n    ax.set_title(\"Signal Wave\")\n    ax.set_xlim(0, len(audio))\n    display.display(display.Audio(np.transpose(audio), rate=16000))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-03-12T08:10:04.845821Z","iopub.execute_input":"2023-03-12T08:10:04.846263Z","iopub.status.idle":"2023-03-12T08:10:10.627639Z","shell.execute_reply.started":"2023-03-12T08:10:04.846224Z","shell.execute_reply":"2023-03-12T08:10:10.626699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def CTCLoss(y_true, y_pred):\n    # Compute the training-time loss value\n    batch_len = tf.cast(tf.shape(y_true)[0], dtype=\"int64\")\n    input_length = tf.cast(tf.shape(y_pred)[1], dtype=\"int64\")\n    label_length = tf.cast(tf.shape(y_true)[1], dtype=\"int64\")\n\n    input_length = input_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n    label_length = label_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n\n    loss = keras.backend.ctc_batch_cost(y_true, y_pred, input_length, label_length)\n    return loss","metadata":{"execution":{"iopub.status.busy":"2023-03-12T08:10:14.893885Z","iopub.execute_input":"2023-03-12T08:10:14.894604Z","iopub.status.idle":"2023-03-12T08:10:14.905657Z","shell.execute_reply.started":"2023-03-12T08:10:14.894554Z","shell.execute_reply":"2023-03-12T08:10:14.904498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_model(input_dim, output_dim, rnn_layers=5, rnn_units=128):\n    \"\"\"Model similar to DeepSpeech2.\"\"\"\n    # Model's input\n    input_spectrogram = layers.Input((None, input_dim), name=\"input\")\n    # Expand the dimension to use 2D CNN.\n    x = layers.Reshape((-1, input_dim, 1), name=\"expand_dim\")(input_spectrogram)\n    # Convolution layer 1\n    x = layers.Conv2D(\n        filters=32,\n        kernel_size=[11, 41],\n        strides=[2, 2],\n        padding=\"same\",\n        use_bias=False,\n        name=\"conv_1\",\n    )(x)\n    x = layers.BatchNormalization(name=\"conv_1_bn\")(x)\n    x = layers.ReLU(name=\"conv_1_relu\")(x)\n    # Convolution layer 2\n    x = layers.Conv2D(\n        filters=32,\n        kernel_size=[11, 21],\n        strides=[1, 2],\n        padding=\"same\",\n        use_bias=False,\n        name=\"conv_2\",\n    )(x)\n    x = layers.BatchNormalization(name=\"conv_2_bn\")(x)\n    x = layers.ReLU(name=\"conv_2_relu\")(x)\n    # Reshape the resulted volume to feed the RNNs layers\n    x = layers.Reshape((-1, x.shape[-2] * x.shape[-1]))(x)\n    # RNN layers\n    for i in range(1, rnn_layers + 1):\n        recurrent = layers.GRU(\n            units=rnn_units,\n            activation=\"tanh\",\n            recurrent_activation=\"sigmoid\",\n            use_bias=True,\n            return_sequences=True,\n            reset_after=True,\n            name=f\"gru_{i}\",\n        )\n        x = layers.Bidirectional(\n            recurrent, name=f\"bidirectional_{i}\", merge_mode=\"concat\"\n        )(x)\n        if i < rnn_layers:\n            x = layers.Dropout(rate=0.5)(x)\n    # Dense layer\n    x = layers.Dense(units=rnn_units * 2, name=\"dense_1\")(x)\n    x = layers.ReLU(name=\"dense_1_relu\")(x)\n    x = layers.Dropout(rate=0.5)(x)\n    # Classification layer\n    output = layers.Dense(units=output_dim + 1, activation=\"softmax\")(x)\n    # Model\n    model = keras.Model(input_spectrogram, output, name=\"DeepSpeech_2\")\n    # Optimizer\n    opt = keras.optimizers.Adam(learning_rate=1e-4)\n    # Compile the model and return\n    model.compile(optimizer=opt, loss=CTCLoss)\n    return model\n\n\n# Get the model\nmodel = build_model(\n    input_dim=fft_length // 2 + 1,\n    output_dim=char_to_num.vocabulary_size(),\n    rnn_units=512,\n)\nmodel.summary(line_length=110)","metadata":{"execution":{"iopub.status.busy":"2023-03-12T08:10:19.872011Z","iopub.execute_input":"2023-03-12T08:10:19.872531Z","iopub.status.idle":"2023-03-12T08:10:24.789515Z","shell.execute_reply.started":"2023-03-12T08:10:19.872476Z","shell.execute_reply":"2023-03-12T08:10:24.788631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# A utility function to decode the output of the network\ndef decode_batch_predictions(pred):\n    input_len = np.ones(pred.shape[0]) * pred.shape[1]\n    # Use greedy search. For complex tasks, you can use beam search\n    results = keras.backend.ctc_decode(pred, input_length=input_len, greedy=True)[0][0]\n    # Iterate over the results and get back the text\n    output_text = []\n    for result in results:\n        result = tf.strings.reduce_join(num_to_char(result)).numpy().decode(\"utf-8\")\n        output_text.append(result)\n    return output_text\n\n\n# A callback class to output a few transcriptions during training\nclass CallbackEval(keras.callbacks.Callback):\n    \"\"\"Displays a batch of outputs after every epoch.\"\"\"\n\n    def __init__(self, dataset):\n        super().__init__()\n        self.dataset = dataset\n\n    def on_epoch_end(self, epoch: int, logs=None):\n        predictions = []\n        targets = []\n        for batch in self.dataset:\n            X, y = batch\n            batch_predictions = model.predict(X)\n            batch_predictions = decode_batch_predictions(batch_predictions)\n            predictions.extend(batch_predictions)\n            for label in y:\n                label = (\n                    tf.strings.reduce_join(num_to_char(label)).numpy().decode(\"utf-8\")\n                )\n                targets.append(label)\n        wer_score = wer(targets, predictions)\n        print(\"-\" * 100)\n        print(f\"Word Error Rate: {wer_score:.4f}\")\n        print(\"-\" * 100)\n        for i in np.random.randint(0, len(predictions), 2):\n            print(f\"Target    : {targets[i]}\")\n            print(f\"Prediction: {predictions[i]}\")\n            print(\"-\" * 100)\n        if(epoch % 3 == 0):\n            numb = str(epoch + 12)\n            model_name = 'model' + numb + '.h5'\n            model.save(model_name)                     ","metadata":{"execution":{"iopub.status.busy":"2023-03-12T08:10:32.521704Z","iopub.execute_input":"2023-03-12T08:10:32.522505Z","iopub.status.idle":"2023-03-12T08:10:32.53621Z","shell.execute_reply.started":"2023-03-12T08:10:32.522466Z","shell.execute_reply":"2023-03-12T08:10:32.534888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the number of epochs.\nepochs = 19\n# Callback function to check transcription on the val set.\nvalidation_callback = CallbackEval(validation_dataset)\n# Train the model\n#model = tf.keras.models.load_model('my_model.h5', custom_objects={'CTCLoss':CTCLoss})\nhistory = model.fit(\n    train_dataset,\n    validation_data=validation_dataset,\n    epochs=epochs,\n    callbacks=[validation_callback],\n)","metadata":{"execution":{"iopub.status.busy":"2023-03-11T12:16:35.558218Z","iopub.execute_input":"2023-03-11T12:16:35.559449Z","iopub.status.idle":"2023-03-11T13:03:18.7703Z","shell.execute_reply.started":"2023-03-11T12:16:35.559407Z","shell.execute_reply":"2023-03-11T13:03:18.769308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#import shutil\n#shutil.rmtree(\"model10.h5\")\n#os.remove(\"/kaggle/working/model10.h5\")\n#model = tf.keras.models.load_model('/kaggle/input/speech-to-text-models/model12.h5', custom_objects={'CTCLoss':CTCLoss})","metadata":{"execution":{"iopub.status.busy":"2023-03-12T08:11:11.810909Z","iopub.execute_input":"2023-03-12T08:11:11.811278Z","iopub.status.idle":"2023-03-12T08:11:19.504134Z","shell.execute_reply.started":"2023-03-12T08:11:11.811245Z","shell.execute_reply":"2023-03-12T08:11:19.502925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's check results on more validation samples\npredictions = []\ntargets = []\nfor batch in validation_dataset:\n    X, y = batch\n    batch_predictions = model.predict(X)\n    batch_predictions = decode_batch_predictions(batch_predictions)\n    predictions.extend(batch_predictions)\n    for label in y:\n        label = tf.strings.reduce_join(num_to_char(label)).numpy().decode(\"utf-8\")\n        targets.append(label)\nwer_score = wer(targets, predictions)\nprint(\"-\" * 100)\nprint(f\"Word Error Rate: {wer_score:.4f}\")\nprint(\"-\" * 100)\nfor i in np.random.randint(0, len(predictions), 10):\n    print(f\"Target    : {targets[i]}\")\n    print(f\"Prediction: {predictions[i]}\")\n    print(\"-\" * 100)","metadata":{"execution":{"iopub.status.busy":"2023-03-12T08:11:26.001111Z","iopub.execute_input":"2023-03-12T08:11:26.001472Z","iopub.status.idle":"2023-03-12T08:15:26.066453Z","shell.execute_reply.started":"2023-03-12T08:11:26.00144Z","shell.execute_reply":"2023-03-12T08:15:26.065356Z"},"trusted":true},"execution_count":null,"outputs":[]}]}